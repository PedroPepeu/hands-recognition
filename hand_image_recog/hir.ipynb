{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_labels = {\n",
    "    1: \"Doing other things\", \n",
    "    2: \"Drumming Fingers\", \n",
    "    3: \"No gesture\", \n",
    "    4: \"Pulling Hand In\", \n",
    "    5: \"Pulling Two Fingers In\", \n",
    "    6: \"Pushing Hand Away\", \n",
    "    7: \"Pushing Two Fingers Away\", \n",
    "    8: \"Rolling Hand Backward\", \n",
    "    9: \"Rolling Hand Forward\", \n",
    "    10: \"Shaking Hand\", \n",
    "    11: \"Sliding Two Fingers Down\",\n",
    "    12: \"Sliding Two Fingers Left\", \n",
    "    13: \"Sliding Two Fingers Right\", \n",
    "    14: \"Sliding Two Fingers Up\", \n",
    "    15: \"Stop Sign\",\n",
    "    16: \"Swiping Down\", \n",
    "    17: \"Swiping Left\", \n",
    "    18: \"Swiping Right\", \n",
    "    19: \"Swiping Up\", \n",
    "    20: \"Thumb Down\",\n",
    "    21: \"Thumb Up\", \n",
    "    22: \"Turning Hand Clockwise\", \n",
    "    23: \"Turning Hand Counterclockwise\", \n",
    "    24: \"Zooming In With Full Hand\",\n",
    "    25: \"Zooming In With Two Fingers\", \n",
    "    26: \"Zooming Out With Full Hand\", \n",
    "    27: \"Zooming Out With Two Fingers\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder_number in range(1, 100000):  # Iterate over folder numbers from 1 to 99999\n",
    "        gesture_name = gesture_labels.get(folder_number, None)  # Get gesture name from mapping\n",
    "        if gesture_name is None:\n",
    "            continue  # Skip folders without a gesture label\n",
    "\n",
    "        gesture_path = os.path.join(dataset_path, str(folder_number))\n",
    "        \n",
    "        if os.path.isdir(gesture_path):  # Ensure it's a directory\n",
    "            for img_file in os.listdir(gesture_path):\n",
    "                img_path = os.path.join(gesture_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                # Preprocess the image (resize, normalize)\n",
    "                img_resized = cv2.resize(img, (64, 64))  # Adjust size as needed\n",
    "                img_normalized = img_resized / 255.0\n",
    "                data.append(img_normalized)\n",
    "                labels.append(folder_number)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "dataset_path = \"../dataset/20bn-jester-v1\"\n",
    "X, y = process_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),  # Added another convolutional layer\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),  # Increased hidden layer size\n",
    "    Dropout(0.5),\n",
    "    Dense(27, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Specify learning rate\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 214ms/step - accuracy: 0.0413 - loss: 3.2959 - val_accuracy: 0.0309 - val_loss: 3.2960\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.0424 - loss: 3.2958 - val_accuracy: 0.0412 - val_loss: 3.2963\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.0386 - loss: 3.2958 - val_accuracy: 0.0412 - val_loss: 3.2964\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - accuracy: 0.0330 - loss: 3.2956 - val_accuracy: 0.0412 - val_loss: 3.2966\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.0394 - loss: 3.2955 - val_accuracy: 0.0412 - val_loss: 3.2970\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.0345 - loss: 3.2954 - val_accuracy: 0.0412 - val_loss: 3.2979\n",
      "Epoch 7/10\n",
      "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - accuracy: 0.0312 - loss: 3.2946"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train_encoded, \n",
    "    epochs=10, \n",
    "    validation_data=(X_test, y_test_encoded), \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gesture_recognition_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
