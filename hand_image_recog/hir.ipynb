{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_labels = {\n",
    "    1: \"DOT\",  # Doing other things\n",
    "    2: \"DF\",   # Drumming Fingers\n",
    "    3: \"NG\",   # No gesture\n",
    "    4: \"PHI\",  # Pulling Hand In\n",
    "    5: \"PTFI\", # Pulling Two Fingers In\n",
    "    6: \"PHA\",  # Pushing Hand Away\n",
    "    7: \"PTFA\", # Pushing Two Fingers Away\n",
    "    8: \"RHB\",  # Rolling Hand Backward\n",
    "    9: \"RHF\",  # Rolling Hand Forward\n",
    "    10: \"SH\",   # Shaking Hand\n",
    "    11: \"STFD\", # Sliding Two Fingers Down\n",
    "    12: \"STFL\", # Sliding Two Fingers Left\n",
    "    13: \"STFR\", # Sliding Two Fingers Right\n",
    "    14: \"STFU\", # Sliding Two Fingers Up\n",
    "    15: \"SS\",   # Stop Sign\n",
    "    16: \"SD\",   # Swiping Down\n",
    "    17: \"SL\",   # Swiping Left\n",
    "    18: \"SR\",   # Swiping Right\n",
    "    19: \"SU\",   # Swiping Up\n",
    "    20: \"TD\",   # Thumb Down\n",
    "    21: \"TU\",   # Thumb Up\n",
    "    22: \"THC\",  # Turning Hand Clockwise\n",
    "    23: \"THCC\", # Turning Hand Counterclockwise\n",
    "    24: \"ZIFH\", # Zooming In With Full Hand\n",
    "    25: \"ZIF2F\",# Zooming In With Two Fingers\n",
    "    26: \"ZOFH\", # Zooming Out With Full Hand\n",
    "    27: \"ZO2F\"  # Zooming Out With Two Fingers\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for folder_number in range(1, 100000):  # Iterate over folder numbers from 1 to 99999\n",
    "        gesture_name = gesture_labels.get(folder_number, None)  # Get gesture name from mapping\n",
    "        if gesture_name is None:\n",
    "            continue  # Skip folders without a gesture label\n",
    "\n",
    "        gesture_path = os.path.join(dataset_path, str(folder_number))\n",
    "        \n",
    "        if os.path.isdir(gesture_path):  # Ensure it's a directory\n",
    "            for img_file in os.listdir(gesture_path):\n",
    "                img_path = os.path.join(gesture_path, img_file)\n",
    "                img = cv2.imread(img_path)\n",
    "\n",
    "                # Preprocess the image (resize, normalize)\n",
    "                img_resized = cv2.resize(img, (64, 64))  # Adjust size as needed\n",
    "                img_normalized = img_resized / 255.0\n",
    "                data.append(img_normalized)\n",
    "                labels.append(folder_number)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "dataset_path = \"../dataset/20bn-jester-v1\"\n",
    "X, y = process_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/Code/hands-recognition/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Conv2D(128, (3,3), activation='relu'),  # Added another convolutional layer\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),  # Increased hidden layer size\n",
    "    Dropout(0.5),\n",
    "    Dense(27, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),  # Specify learning rate\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - accuracy: 0.0550 - loss: 3.2686 - val_accuracy: 0.1392 - val_loss: 3.1665\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.2820 - loss: 3.0921 - val_accuracy: 0.5052 - val_loss: 2.8262\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.4068 - loss: 2.6415 - val_accuracy: 0.7680 - val_loss: 1.9095\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.6493 - loss: 1.7159 - val_accuracy: 0.9381 - val_loss: 0.7924\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.8129 - loss: 0.8539 - val_accuracy: 1.0000 - val_loss: 0.2565\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.9345 - loss: 0.3725 - val_accuracy: 1.0000 - val_loss: 0.1037\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.9484 - loss: 0.2197 - val_accuracy: 1.0000 - val_loss: 0.0444\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.9813 - loss: 0.1257 - val_accuracy: 1.0000 - val_loss: 0.0225\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - accuracy: 0.9830 - loss: 0.0933 - val_accuracy: 1.0000 - val_loss: 0.0140\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.9881 - loss: 0.0679 - val_accuracy: 1.0000 - val_loss: 0.0103\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train_encoded, \n",
    "    epochs=10, \n",
    "    validation_data=(X_test, y_test_encoded), \n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0088\n",
      "Test Accuracy: 1.00\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're using the encoded test labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Optional: Confusion Matrix for deeper insights\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_test_encoded, y_pred_classes))\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gesture_recognition_model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
